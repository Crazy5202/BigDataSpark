{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Общая настройка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars ./jars/postgresql-42.6.0.jar,./jars/clickhouse-jdbc-0.4.6.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DB_NAME = os.getenv(\"POSTGRES_DB\")\n",
    "DB_USER = os.getenv(\"POSTGRES_USER\")\n",
    "DB_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"POSTGRES_HOST\")\n",
    "DB_PORT = os.getenv(\"POSTGRES_PORT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание таблиц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def create_tables():\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD,\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS d_customer (\n",
    "                customer_id SERIAL PRIMARY KEY,\n",
    "                customer_first_name TEXT,\n",
    "                customer_last_name TEXT,\n",
    "                customer_email TEXT,\n",
    "                customer_age INTEGER,\n",
    "                customer_country TEXT,\n",
    "                customer_postal_code TEXT,\n",
    "                customer_pet_name TEXT,\n",
    "                customer_pet_type TEXT,\n",
    "                customer_pet_breed TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS d_supplier (\n",
    "                supplier_id SERIAL PRIMARY KEY,\n",
    "                supplier_name TEXT,\n",
    "                supplier_email TEXT,\n",
    "                supplier_phone TEXT,\n",
    "                supplier_contact TEXT,\n",
    "                supplier_country TEXT,\n",
    "                supplier_city TEXT,\n",
    "                supplier_address TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS d_product (\n",
    "                product_id SERIAL PRIMARY KEY,\n",
    "                product_name TEXT,\n",
    "                product_category TEXT,\n",
    "                pet_category TEXT,\n",
    "                product_brand TEXT,\n",
    "                product_price DECIMAL,\n",
    "                product_quantity INTEGER,\n",
    "                product_size TEXT,\n",
    "                product_weight DECIMAL,\n",
    "                product_color TEXT,\n",
    "                product_material TEXT,\n",
    "                product_description TEXT,\n",
    "                product_rating DECIMAL,\n",
    "                product_reviews INTEGER,\n",
    "                product_release_date DATE,\n",
    "                product_expiry_date DATE,\n",
    "                supplier_id INTEGER REFERENCES d_supplier(supplier_id)\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS d_seller (\n",
    "                seller_id SERIAL PRIMARY KEY,\n",
    "                seller_first_name TEXT,\n",
    "                seller_last_name TEXT,\n",
    "                seller_email TEXT,\n",
    "                seller_country TEXT,\n",
    "                seller_postal_code TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS d_store (\n",
    "                store_id SERIAL PRIMARY KEY,\n",
    "                store_name TEXT,\n",
    "                store_location TEXT,\n",
    "                store_city TEXT,\n",
    "                store_state TEXT,\n",
    "                store_country TEXT,\n",
    "                store_email TEXT,\n",
    "                store_phone TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS f_sales (\n",
    "                sale_id SERIAL PRIMARY KEY,\n",
    "                sale_date DATE,\n",
    "                sale_customer_id INTEGER REFERENCES d_customer(customer_id),\n",
    "                sale_product_id INTEGER REFERENCES d_product(product_id),\n",
    "                sale_seller_id INTEGER REFERENCES d_seller(seller_id),\n",
    "                sale_store_id INTEGER REFERENCES d_store(store_id),\n",
    "                sale_quantity INTEGER,\n",
    "                sale_total_price DECIMAL\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Перевод данных в звезду"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание соединения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "25/05/24 19:32:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL with PostgreSQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Настройки соединения и считывание основной таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = f\"jdbc:postgresql://{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "properties = {\n",
    "    \"user\": DB_USER,\n",
    "    \"password\": DB_PASSWORD,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "source_table = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"mock_data\",\n",
    "    properties=properties\n",
    ").withColumn(\"sale_date\", to_date(\"product_release_date\", \"m/d/yyyy\")) \\\n",
    ".withColumn(\"product_release_date\", to_date(\"product_release_date\", \"m/d/yyyy\")) \\\n",
    ".withColumn(\"product_expiry_date\", to_date(\"product_expiry_date\", \"m/d/yyyy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Таблицы измерений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Покупатели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/24 19:32:46 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customers = source_table.select([\n",
    "    \"customer_first_name\",\n",
    "    \"customer_last_name\",\n",
    "    \"customer_email\",\n",
    "    \"customer_age\",\n",
    "    \"customer_country\",\n",
    "    \"customer_postal_code\",\n",
    "    \"customer_pet_name\",\n",
    "    \"customer_pet_type\",\n",
    "    \"customer_pet_breed\",\n",
    "]).distinct()\n",
    "\n",
    "customers.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"d_customer\",\n",
    "    mode=\"append\",\n",
    "    properties=properties\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поставщики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppliers = source_table.select([\n",
    "    \"supplier_name\",\n",
    "    \"supplier_email\",\n",
    "    \"supplier_phone\",\n",
    "    \"supplier_contact\",\n",
    "    \"supplier_country\",\n",
    "    \"supplier_city\",\n",
    "    \"supplier_address\"\n",
    "]).distinct()\n",
    "\n",
    "suppliers.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"d_supplier\",\n",
    "    mode=\"append\",\n",
    "    properties=properties\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Продукты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppliers_with_id = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"d_supplier\",\n",
    "    properties=properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "products = source_table.join(suppliers_with_id, on=\"supplier_email\").select([\n",
    "    \"product_name\",\n",
    "    \"product_category\",\n",
    "    \"pet_category\",\n",
    "    \"product_brand\",\n",
    "    \"product_price\",\n",
    "    \"product_quantity\",\n",
    "    \"product_size\",\n",
    "    \"product_weight\",\n",
    "    \"product_color\",\n",
    "    \"product_material\",\n",
    "    \"product_description\",\n",
    "    \"product_rating\",\n",
    "    \"product_reviews\",\n",
    "    \"product_release_date\",\n",
    "    \"product_expiry_date\",\n",
    "    \"supplier_id\"\n",
    "]).distinct()\n",
    "\n",
    "products.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"d_product\",\n",
    "    mode=\"append\",\n",
    "    properties=properties\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Продавцы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers = source_table.select([\n",
    "    \"seller_first_name\",\n",
    "    \"seller_last_name\",\n",
    "    \"seller_email\",\n",
    "    \"seller_country\",\n",
    "    \"seller_postal_code\"\n",
    "]).distinct()\n",
    "\n",
    "sellers.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"d_seller\",\n",
    "    mode=\"append\",\n",
    "    properties=properties\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Магазины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = source_table.select([\n",
    "    \"store_name\",  \n",
    "    \"store_location\",  \n",
    "    \"store_city\",  \n",
    "    \"store_state\",  \n",
    "    \"store_country\",  \n",
    "    \"store_email\",  \n",
    "    \"store_phone\"\n",
    "]).distinct()\n",
    "\n",
    "stores.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"d_store\",\n",
    "    mode=\"append\",\n",
    "    properties=properties\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Таблица фактов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_with_id = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"d_customer\",\n",
    "    properties=properties\n",
    ")\n",
    "\n",
    "products_with_id = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"d_product\",\n",
    "    properties=properties\n",
    ")\n",
    "\n",
    "sellers_with_id = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"d_seller\",\n",
    "    properties=properties\n",
    ")\n",
    "\n",
    "stores_with_id = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"d_store\",\n",
    "    properties=properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = source_table \\\n",
    ".join(customers_with_id, on=\"customer_email\") \\\n",
    ".join(products_with_id, on=[\"product_name\",\n",
    "    \"product_category\",\n",
    "    \"pet_category\",\n",
    "    \"product_brand\",\n",
    "    \"product_price\",\n",
    "    \"product_quantity\",\n",
    "    \"product_size\",\n",
    "    \"product_weight\",\n",
    "    \"product_color\",\n",
    "    \"product_material\",\n",
    "    \"product_description\",\n",
    "    \"product_rating\",\n",
    "    \"product_reviews\",\n",
    "    \"product_release_date\",\n",
    "    \"product_expiry_date\",]) \\\n",
    ".join(sellers_with_id, on=\"seller_email\")\\\n",
    ".join(stores_with_id, on =\"store_email\") \\\n",
    ".selectExpr([\n",
    "    \"sale_date\",\n",
    "    \"customer_id as sale_customer_id\",\n",
    "    \"product_id as sale_product_id\",\n",
    "    \"seller_id as sale_seller_id\",\n",
    "    \"store_id as sale_store_id\",\n",
    "    \"sale_quantity\",\n",
    "    \"sale_total_price\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o191.count.\n: org.apache.spark.SparkException: [INTERNAL_ERROR] The \"count\" action failed. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace. SQLSTATE: XX000\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.count(Dataset.scala:1499)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\n\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics$lzycompute(HashAggregateExec.scala:71)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics(HashAggregateExec.scala:70)\n\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.resetMetrics(AdaptiveSparkPlanExec.scala:245)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2233)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t... 28 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msales\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/classic/dataframe.py:439\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o191.count.\n: org.apache.spark.SparkException: [INTERNAL_ERROR] The \"count\" action failed. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace. SQLSTATE: XX000\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$.toInternalError(QueryExecution.scala:643)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:656)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.count(Dataset.scala:1499)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.sql.classic.SparkSession.sparkContext()\" because the return value of \"org.apache.spark.sql.execution.SparkPlan.session()\" is null\n\tat org.apache.spark.sql.execution.SparkPlan.sparkContext(SparkPlan.scala:68)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics$lzycompute(HashAggregateExec.scala:71)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.metrics(HashAggregateExec.scala:70)\n\tat org.apache.spark.sql.execution.SparkPlan.resetMetrics(SparkPlan.scala:147)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.resetMetrics(AdaptiveSparkPlanExec.scala:245)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2233)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\t... 28 more\n"
     ]
    }
   ],
   "source": [
    "sales.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"f_sales\",\n",
    "    mode=\"append\",\n",
    "    properties=properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
